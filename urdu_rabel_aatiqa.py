# -*- coding: utf-8 -*-
"""urdu@rabel

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hpaw_rIcZE8A_g3gnybLIwgMsGmRUr05
"""

# load data and take a quick look
import re
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer

from google.colab import drive
drive.mount('/content/drive')

import os
os.chdir("/content/drive/My Drive/gans-urdu-news/dataset/")

raw_data = pd.read_csv('urdu-news-dataset-1M.csv')
#raw_data = pd.read_csv('urdu_rabel.csv')
raw_data.head(5)

raw_data.drop(columns=raw_data[['News Text', 'Date', 'Index', 'URL', 'Source', 'News length']], inplace=True)
#raw_data = raw_data.sample(1000)

raw_data.shape

raw_data['Category'].value_counts()

#raw_data.to_csv("urdu_rabel.csv")

# check the size of the data and its class distribution
sentences = raw_data['Headline'].tolist()
sentiments = raw_data['Category'].tolist()

sns.countplot(x='Category', data=raw_data)
plt.show()

# text cleaning and pre-processing:
def delete_urdu_english_symbols(sentences):
    cleaned = []
    for sentence in sentences:
        text = re.sub(r"\d+", " ", sentence)
        # English punctuations
        text = re.sub(r"""[!"#$%&'()*+,-./:;<=>?@[\]^_`{|}~]+""", " ", text)
        # Urdu punctuations
        text = re.sub(r"[:؛؟’‘٭ء،۔]+", " ", text)
        # Arabic numbers
        text = re.sub(r"[٠‎١‎٢‎٣‎٤‎٥‎٦‎٧‎٨‎٩]+", " ", text)
        text = re.sub(r"[^\w\s]", " ", text)
        # Remove English characters and numbers.
        text = re.sub(r"[a-zA-z0-9]+", " ", text)
        # remove multiple spaces.
        text = re.sub(r" +", " ", text)
        text = text.split(" ")
        # some stupid empty tokens should be removed.
        text = [t.strip() for t in text if t.strip()]
        cleaned.append(" ".join(text))
    return cleaned

X = delete_urdu_english_symbols(sentences)
Y = sentiments

# Feel free to use different ratios to split the data.
train_text, test_text, train_labels, test_labels = train_test_split(X, Y, test_size=0.20, random_state=42)

# training: tf-idf + logistic regression
max_feature_num = 5000
train_vectorizer = TfidfVectorizer(max_features=max_feature_num)
train_vecs = train_vectorizer.fit_transform(train_text)
test_vecs = TfidfVectorizer(max_features=max_feature_num,vocabulary=train_vectorizer.vocabulary_).fit_transform(test_text)

# train model
from sklearn.linear_model import LogisticRegression
clf = LogisticRegression().fit(train_vecs, train_labels)

# test model
test_pred = clf.predict(test_vecs)
from sklearn.metrics import precision_recall_fscore_support,accuracy_score
acc = accuracy_score(test_labels, test_pred)
pre, rec, f1, _ = precision_recall_fscore_support(test_labels, test_pred, average='macro')
print('acc', acc)
print('precision', pre)
print('rec', rec)
print('f1', f1)

import pickle

# save model and other necessary modules
all_info_want_to_save = {
    'model': clf,
    'vectorizer': TfidfVectorizer(max_features=max_feature_num,vocabulary=train_vectorizer.vocabulary_)
}
save_path = open("sentiment_urdu_logistic_regression.pickle","wb")
pickle.dump(all_info_want_to_save, save_path)

import pickle

# reload your model and use it to make predictions for test text
# you should adjust the code so as to load to your saved model/components
def test_trained_model(model_path, test_text):
    saved_model_dic = pickle.load(open(model_path,"rb"))
    saved_clf = saved_model_dic['model']
    saved_vectorizer = saved_model_dic['vectorizer']
    print(len(saved_vectorizer.vocabulary))
    new_test_vecs = saved_vectorizer.fit_transform(test_text)
    return saved_clf.predict(new_test_vecs)


# load sample test data
import pandas as pd
# provide csv file to test
test_data = pd.read_csv('test_data.csv')
test_text = test_data['text'].tolist()[-5000:]
test_labels = test_data['sentiment'].tolist()[-5000:]

print('test data size', len(test_labels))

# test model
from sklearn.metrics import precision_recall_fscore_support,accuracy_score
new_test_pred = test_trained_model("sentiment_urdu_logistic_regression.pickle", test_text)
acc = accuracy_score(test_labels, new_test_pred)
pre, rec, f1, _ = precision_recall_fscore_support(test_labels, new_test_pred, average='macro')
print('acc', acc)
print('precision', pre)
print('rec', rec)
print('f1', f1)

